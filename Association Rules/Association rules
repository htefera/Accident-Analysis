{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Association rules","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"FllH47XeSGP3","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Libraries for importing datasets from Google Drive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"axm_Un_0SzH6","colab_type":"code","colab":{}},"source":["# Configuring Google Drive file loading (run it only once)\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7BYu4SKS1NX","colab_type":"code","colab":{}},"source":["link = 'https://drive.google.com/open?id=1MURORv9iCRNNZtZORoNlv3WtDAYcGYqJ'\n","id = link.split('=')[1]\n","\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile(id)  \n","df = pd.read_csv(id, sep=',')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_kOEe8niTexj","colab_type":"code","colab":{}},"source":["df2 = df.loc[df['Accident_Severity'] == 2].sample(n=2500, random_state=1)\n","#print(df2.shape)\n","df2 = pd.concat([\n","    df2,df.loc[df['Accident_Severity'] == 3].sample(n=2500, random_state=1)\n","],ignore_index=True)\n","#print(df2.shape)\n","\n","df2 = pd.concat([\n","    df2,df.loc[df['Accident_Severity'] == 1]\n","],ignore_index=True)\n","#print(df2.shape)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ePSCJIJfH_eC","colab_type":"text"},"source":["Downsampling"]},{"cell_type":"code","metadata":{"id":"_9Z4GHTHvWxy","colab_type":"code","colab":{}},"source":["import random\n","import time\n","\n","def create_dataset(dataset):\n","\trandom.seed(time.time())\n","\ttmp = dataset.loc[dataset['Accident_Severity'] == 2].sample(n=2500, random_state = random.randrange(0,1000))\n","\t\n","\ttmp = pd.concat([\n","\t\ttmp,dataset.loc[dataset['Accident_Severity'] == 3].sample(n=2500, random_state = random.randrange(0,1000))\n","\t],ignore_index=True)\n","\t\n","\t\n","\ttmp = pd.concat([\n","\t\ttmp,dataset.loc[dataset['Accident_Severity'] == 1]\n","\t],ignore_index=True)\n","\tprint(tmp.shape)\n","\treturn tmp"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8t8E73mVH3Iv","colab_type":"text"},"source":["Preprocessing functions"]},{"cell_type":"code","metadata":{"id":"Qh3o44QRueI5","colab_type":"code","colab":{}},"source":["#remove Unknown for Road_Type, Weather_Conditions\n","#remove nan for Road_Surface_Conditions\n","#remove None for Special_Conditions_at_Site, Carriageway_Hazards\n","\n","def remove_nones(l):\n","\tlength1 = len(l)\n","\tj=0\n","\twhile(j < length1):\n","\t\ti = 0\n","\t\tlength = len(l[j])  #list length\n","\t\t#print(row) \n","\t\twhile(i<length):\n","\t\t\tif(l[j][i]=='None' or l[j][i]=='Unknown' or l[j][i] is np.nan or l[j][i]=='nan'):\n","\t\t\t\tl[j].remove (l[j][i])\n","\t\t\t\tlength = length -1  \n","\t\t\t\tcontinue\n","\t\t\ti = i+1\n","\t\tj = j+1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S-UcX5md79t7","colab_type":"code","colab":{}},"source":["def select_features(l):\n","\tl['Accident_Severity'] = l['Accident_Severity'].map({1: 'Fatal', 2: 'Serious', 3: 'Slight'})\n","\tl['Day_of_Week'] = l['Day_of_Week'].map({1: 'Sun', 2: 'Mon', 3: 'Tus', 4: 'Wed', 5: 'Thu', 6: 'Fri', 7: 'Sat'})\n","\tl['Speed_limit'] = l['Speed_limit'].map({20: 'speed_20', 30: 'speed_30', 40: 'speed_40', 50: 'speed_50', 60: 'speed_60', 70: 'speed_70'})\n","\tl['Urban_or_Rural_Area'] = l['Urban_or_Rural_Area'].map({1: 'Urban', 2: 'Rural'})\t\n","\t#Discretize time\n","\thours = pd.to_datetime(l['Time'], format='%H:%M').dt.hour\n","\tl['Time'] = pd.cut(hours, \n","                    bins=[0,6,12,18,24], \n","                    include_lowest=True, \n","                    labels=['Midnight','Morning','Evening','Night'])\n","\tl['month'] = l['month'].map({1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'})\n","\treturn l"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qwtm0x8OoPc0","colab_type":"code","colab":{}},"source":["def preprocess_dateset_for_assossiation_rules(dataset):\n","\t# removing useless columns (more than 90% of the data have the same value)\n","\tdataset.drop('Special_Conditions_at_Site', axis = 1,inplace = True)\n","\tdataset.drop('Carriageway_Hazards',axis = 1,inplace = True)\n","\tdataset.drop('Pedestrian_Crossing-Human_Control',axis = 1, inplace= True)\n","\t# removing duplicate of coordinate system\n","\tdataset.drop('Location_Northing_OSGR',axis = 1, inplace = True)\n","\tdataset.drop('Location_Easting_OSGR',axis = 1,inplace = True)\n","\tdataset.drop('Police_Force',axis = 1,inplace = True)\n","\tdataset.drop('Number_of_Vehicles',axis = 1,inplace = True)\n","\tdataset.drop('Number_of_Casualties',axis = 1,inplace = True)\n","\tdataset.drop('Did_Police_Officer_Attend_Scene_of_Accident',axis = 1,inplace = True)\n","\tdataset.drop('Accident_Index',axis = 1,inplace = True)\n","\tdataset['month'] = pd.DatetimeIndex(dataset['Date']).month\n","\tdataset.drop('Date',axis = 1,inplace = True)\n","\tdataset.drop('Junction_Detail',axis = 1,inplace = True)\n"," \n","\t#to be used later\n","\tdataset.drop('Longitude',axis = 1,inplace = True)\n","\tdataset.drop('Latitude',axis = 1,inplace = True)\n","\tdataset.drop('1st_Road_Number',axis = 1,inplace = True)\n","\tdataset.drop('2nd_Road_Number',axis = 1,inplace = True)\n","\tdataset.drop('Year',axis = 1,inplace = True)\n","\t\n","\tdataset['Local_Authority_(District)'] = 'local_auth_' + df['Local_Authority_(District)'].astype(str)\n","\tdataset['1st_Road_Class'] = '1st_road_class_' + df['1st_Road_Class'].astype(str)\n","\tdataset['2nd_Road_Class'] = '2nd_road_class_' + df['2nd_Road_Class'].astype(str)\n","\treturn select_features(dataset)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_eRYakgK4oIl","colab_type":"code","colab":{}},"source":["def preprocess_dateset_for_assossiation_rules2(dataset):\n","  # for every column if it contains just one value (without add Nan to the count) drop it\n","  for col in dataset:\n","    if not dataset[col].value_counts().to_list():\n","      dataset.drop(col, axis = 1,inplace = True)\n","\n","  # removig the index\n","  dataset.drop('Accident_Index',axis = 1, inplace = True)\n","  # removing useless columns (more than 90% of the data have the same value)\n","  dataset.drop('Special_Conditions_at_Site', axis = 1,inplace = True)\n","  dataset.drop('Carriageway_Hazards',axis = 1,inplace = True)\n","  dataset.drop('Pedestrian_Crossing-Human_Control',axis = 1, inplace= True)\n","  # removing duplicate of coordinate system\n","  dataset.drop('Location_Northing_OSGR',axis = 1, inplace = True)\n","  dataset.drop('Location_Easting_OSGR',axis = 1,inplace = True)\n","  dataset.drop('LSOA_of_Accident_Location',axis = 1,inplace = True)\n","  dataset.drop('1st_Road_Number',axis = 1, inplace = True)\n","  dataset.drop('2nd_Road_Number',axis = 1, inplace = True)\n","  dataset.drop('Police_Force',axis = 1, inplace = True)\n","  dataset.drop('Year',axis = 1, inplace = True)\n","\n","  # removing junctioncontrol because it has 40% of nan values\n","  dataset.drop('Junction_Control',axis = 1,inplace = True)\n","  # REMOVE ALSO THE FIRST AND SECOND ORDER STUFF BECAUSE THEY ARE USEFUL ONLY FOR JUNCTIONS\n","\n","  # removing the date because we just use the day of the week\n","  dataset['month'] = pd.DatetimeIndex(dataset['Date']).month\n","  dataset.drop('Date',inplace = True, axis = 1)\n","\n","  # removing 13 nan rows in time, no way to impute it\n","  dataset.dropna(subset = ['Time'],inplace = True)\n","  dataset.dropna(subset = ['Did_Police_Officer_Attend_Scene_of_Accident'],inplace = True)\n","\n","  # merging attribute values\n","  dataset.loc[dataset['Number_of_Vehicles'] > 2,'Number_of_Vehicles'] = '3+'\n","  dataset.loc[dataset['Number_of_Vehicles'] == 2,'Number_of_Vehicles'] = 'two'\n","  dataset.loc[dataset['Number_of_Vehicles'] == 1,'Number_of_Vehicles'] = 'one'\n","  dataset.loc[(dataset['Road_Type'] != 'Single carriageway') & (dataset['Road_Type'] != 'Dual carriageway'),'Road_Type'] = 'Other'\n","  dataset.loc[dataset['Pedestrian_Crossing-Physical_Facilities'] != 'No physical crossing within 50 meters', 'Pedestrian_Crossing-Physical_Facilities'] = 'Physical Crossing present'\n","  dataset.loc[(dataset['Weather_Conditions'] != 'Fine without high winds') & (dataset['Weather_Conditions'] != 'Raining without high winds'), 'Weather_Conditions'] = 'Other'\n","  dataset.loc[(dataset['Road_Surface_Conditions'] != 'Dry') & (dataset['Road_Surface_Conditions'] != 'Wet/Damp'),'Road_Surface_Conditions'] = 'Extreme_condition' \n","  dataset.loc[(dataset['Number_of_Casualties'] != 1) & (dataset['Number_of_Casualties'] != 2),'Number_of_Casualties'] = '3+'\n","  dataset.loc[dataset['Number_of_Casualties'] == 2,'Number_of_Casualties'] = 'two'\n","  dataset.loc[dataset['Number_of_Casualties'] == 1,'Number_of_Casualties'] = 'one' \n","  dataset.loc[(dataset['Light_Conditions'] == 'Daylight: Street light present') | (dataset['Light_Conditions'] == 'Darkness: Street lights present and lit'),'Light_Conditions'] = 'Proper lightning'\n","  dataset.loc[(dataset['Light_Conditions'] == 'Darkness: Street lights present but unlit') | (dataset['Light_Conditions'] == 'Darkeness: No street lighting'),'Light_Conditions'] = 'Insufficient lightning'\n","  dataset.loc[dataset['Light_Conditions'] == 'Darkness: Street lighting unknown','Light_Conditions'] = 'Unknown'\n","  dataset.loc[dataset['Speed_limit'] == 10,'Speed_limit'] = 20\n","\n","  dataset['Local_Authority_(District)'] = 'local_auth_' + df['Local_Authority_(District)'].astype(str)\n","  dataset['1st_Road_Class'] = '1st_road_class_' + df['1st_Road_Class'].astype(str)\n","  dataset['2nd_Road_Class'] = '2nd_road_class_' + df['2nd_Road_Class'].astype(str)\n","  dataset['Number_of_Casualties'] = 'Number_of_Casualties = ' + df['Number_of_Casualties'].astype(str)\n","  dataset['Number_of_Vehicles'] = 'Number_of_Vehicles = ' + df['Number_of_Vehicles'].astype(str)\n","  dataset['Did_Police_Officer_Attend_Scene_of_Accident'] = 'Did_Police_Officer_Attend_Scene_of_Accident = ' + df['Did_Police_Officer_Attend_Scene_of_Accident'].astype(str)\n","  return select_features(dataset)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vEt_m3fuHygq","colab_type":"text"},"source":["MAIN PROGRAM"]},{"cell_type":"code","metadata":{"id":"ls6O5YaKTyCH","colab_type":"code","colab":{}},"source":["!pip install pyfpgrowth\n","!pip install pygeohash\n","\n","import pyfpgrowth\n","import numpy as np\n","import pygeohash as gh\n","\n","#Transform Latitude and Longitude using geohash\n","df['geohash']=df.apply(lambda x: gh.encode(x.Latitude, x.Longitude, precision=3), axis=1)\n","\n","rules_list = []\n","for iters in range(50):\n","\tprint(\"iteration \"+str(iters)+\" started...\")\n","\tdf2 = create_dataset(df)\n","\tdf3 = preprocess_dateset_for_assossiation_rules(df2)\n","\tdf3 = df3.applymap(str)\n","\t\n","\tdf3_list = df3.values.tolist()\n","\tremove_nones(df3_list)\t\n","\t\n","\tpatterns = pyfpgrowth.find_frequent_patterns(df3_list, 700)\n","\trules = pyfpgrowth.generate_association_rules(patterns, 0.7)\n","\t\n","\trules_list.append(rules)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9hCqK7nohyzI","colab_type":"text"},"source":["Create an dictionary that contains the association rules and their frequency"]},{"cell_type":"code","metadata":{"id":"y52SEhR6Qxm6","colab_type":"code","colab":{}},"source":["rules_dict = {}\n","for rules in rules_list:\n","\tprint(\"next_run......................\")\n","\tfor i in rules:\n","\t\tif('Fatal' in rules[i][0]):\n","\t\t\tif(i in rules_dict):\n","\t\t\t\trules_dict[i] += 1\n","\t\t\telse:\n","\t\t\t\trules_dict[i] = 1\n","\t\t\tprint(str(i) + ' -> ' +str(rules[i][0])+' '+str(rules[i][1]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TQQWaaoHiJ7m","colab_type":"text"},"source":["Print the detected association rules"]},{"cell_type":"code","metadata":{"id":"SZ-dUAWygUX6","colab_type":"code","colab":{}},"source":["for key, value in rules_dict.items() :\n","    print (key, value)"],"execution_count":0,"outputs":[]}]}