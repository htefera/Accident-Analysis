# -*- coding: utf-8 -*-
"""Random Forest (Ricardo)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y3CgZFAZkCMxx_xMYYLwxlx3akysuhsh
"""

import numpy as np
import pandas as pd
from IPython.core.interactiveshell import InteractiveShell
import matplotlib.pyplot as plt
# Libraries for importing datasets from Google Drive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from sklearn import preprocessing
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from time import time

InteractiveShell.ast_node_interactivity = "all"

# Configuring Google Drive file loading (run it only once)
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

link = 'https://drive.google.com/open?id=1MURORv9iCRNNZtZORoNlv3WtDAYcGYqJ'
id = link.split('=')[1]

downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile(id)  
imported_df = pd.read_csv(id, sep=',')

"""What we know:
the db is really skewed, a lot of 3 accident and a very few number of 1

Import the db into *imported_db* and create a shallow copy into *df*
"""

df = imported_df.copy(deep = False)

# CHECK THE CORRELATION MATRIX AND RETURN THE ATTRIBUTES THAT ARE CORRELATED 0.8 OR LESS THAN -0.8
c = df.corr().abs()
s = c.unstack()
so = s.sort_values(kind="quicksort")
so.loc[((so>0.8) & (so < 1)) | ((so < -0.8))]

"""Remove columns that don't fit the purpose of our analysis"""

# for every column if it contains just one value (without add Nan to the count) drop it
for col in df:
  if not df[col].value_counts().to_list():
    df.drop(col, axis = 1,inplace = True)

# removig the index
df.drop('Accident_Index',axis = 1, inplace = True)
# removing useless columns (more than 90% of the data have the same value)
df.drop('Special_Conditions_at_Site', axis = 1,inplace = True)
df.drop('Carriageway_Hazards',axis = 1,inplace = True)
df.drop('Pedestrian_Crossing-Human_Control',axis = 1, inplace= True)
# removing duplicate of coordinate system
df.drop('Location_Northing_OSGR',axis = 1, inplace = True)
df.drop('Location_Easting_OSGR',axis = 1,inplace = True)
df.drop('LSOA_of_Accident_Location',axis = 1,inplace = True)
df.drop('1st_Road_Class',axis = 1, inplace = True)
df.drop('1st_Road_Number',axis = 1, inplace = True)
df.drop('2nd_Road_Class',axis = 1, inplace = True)
df.drop('2nd_Road_Number',axis = 1, inplace = True)
df.drop('Police_Force',axis = 1, inplace = True)
df.drop('Local_Authority_(District)',axis = 1,inplace = True)

# removing junctioncontrol because it has 40% of nan values
df.drop('Junction_Control',axis = 1,inplace = True)
# REMOVE ALSO THE FIRST AND SECOND ORDER STUFF BECAUSE THEY ARE USEFUL ONLY FOR JUNCTIONS

# removing the date because we just use the day of the week
df.drop('Date',inplace = True, axis = 1)

# removing 13 nan rows in time, no way to impute it
df.dropna(subset = ['Time'],inplace = True)
df.dropna(subset = ['Did_Police_Officer_Attend_Scene_of_Accident'],inplace = True)

# merging attribute values
df.loc[df['Number_of_Vehicles'] > 2,'Number_of_Vehicles'] = '3+'
df.loc[df['Number_of_Vehicles'] == 2,'Number_of_Vehicles'] = 'two'
df.loc[df['Number_of_Vehicles'] == 1,'Number_of_Vehicles'] = 'two'
df.loc[(df['Road_Type'] != 'Single carriageway') & (df['Road_Type'] != 'Dual carriageway'),'Road_Type'] = 'Other'
df.loc[df['Pedestrian_Crossing-Physical_Facilities'] != 'No physical crossing within 50 meters', 'Pedestrian_Crossing-Physical_Facilities'] = 'Physical Crossing present'
df.loc[(df['Weather_Conditions'] != 'Fine without high winds') & (df['Weather_Conditions'] != 'Raining without high winds'), 'Weather_Conditions'] = 'Other'
df.loc[(df['Road_Surface_Conditions'] != 'Dry') & (df['Road_Surface_Conditions'] != 'Wet/Damp'),'Road_Surface_Conditions'] = 'Extreme_condition' 
df.loc[(df['Number_of_Casualties'] != 1) & (df['Number_of_Casualties'] != 2),'Number_of_Casualties'] = '3+'
df.loc[df['Number_of_Casualties'] == 2,'Number_of_Casualties'] = 'two'
df.loc[df['Number_of_Casualties'] == 1,'Number_of_Casualties'] = 'two' 
df.loc[(df['Light_Conditions'] == 'Daylight: Street light present') | (df['Light_Conditions'] == 'Darkness: Street lights present and lit'),'Light_Conditions'] = 'Proper lightning'
df.loc[(df['Light_Conditions'] == 'Darkness: Street lights present but unlit') | (df['Light_Conditions'] == 'Darkeness: No street lighting'),'Light_Conditions'] = 'Insufficient lightning'
df.loc[df['Light_Conditions'] == 'Darkness: Street lighting unknown','Light_Conditions'] = 'Unknown'
df.loc[df['Speed_limit'] == 10,'Speed_limit'] = 20
hours = pd.to_datetime(df['Time'], format='%H:%M').dt.hour
df['Time'] = pd.cut(hours, 
                    bins=[0,6,12,18,24], 
                    include_lowest=True, 
                    labels=['Midnight','Morning','Evening','Night'])

# TODO -> DEAL WITH LAT AND LON (Dimitris)

# DISCRETIZE COLUMNS
df['Day_of_Week'] = df['Day_of_Week'].apply(str) # categorical
df['Time'] = df['Time'].apply(str) # categorical
df['Road_Type'] = df['Road_Type'].apply(str) # categorical
df['Speed_limit'] = df['Speed_limit'].apply(str) # categorical
df['Urban_or_Rural_Area'] = df['Urban_or_Rural_Area'].apply(str) # categorical
df['Year'] = df['Year'].apply(str) # categorical

features = df.drop('Accident_Severity',axis = 1).copy()

# Salvo le variabili categoriali
categorical_features = features.select_dtypes(include = ['object','category']).columns

# One Hot Encoding delle feature categoriali 
categorical_transformer = Pipeline(steps = [
    ('onehot', preprocessing.OneHotEncoder())
])

numerical_features = features.select_dtypes(exclude = ['object','category']).columns


numerical_transformer = Pipeline(steps = [
    ('scaler',preprocessing.StandardScaler())
])

# Aggiungo le pipeline in un unico processo preprocessor
preprocessor = ColumnTransformer( transformers = [
    ('num',numerical_transformer, numerical_features),                                 
    ('cat',categorical_transformer, categorical_features)
])

# Creo un modello che contenga tutte le feature possibili -> massima varianza
transformation = make_pipeline(
    preprocessor
)

transformation.fit(features)

# split in train and test
train_df, test_df = train_test_split(df,test_size = 0.2)
X_train = train_df.drop('Accident_Severity',axis = 1).copy()
y_train = train_df['Accident_Severity'].copy()
X_test = test_df.drop('Accident_Severity',axis = 1).copy()
y_test = test_df['Accident_Severity'].copy()

y_train[y_train != 3] = 1
y_train[y_train == 3] = 0
y_test[y_test != 3] = 1
y_test[y_test == 3] = 0

# merge the fatal and the serious together
#y_train[y_train != 3] = 2
#y_test[y_test != 3] = 2

"""#HOW TO RUN A MODEL

Assign the model with all the parameter that you want to add to a variable. 

```
RFClassifier = RandomForestClassifier(n_estimators= 10, max_depth= 2)
```

Fit the classifier using the transformation of X:

```
RFClassifier.fit(transformation.transform(X_train),y_train)
```

In this way what is happening is that X_train is transformed with OneHotEncoding and then this tranformation is ingested by the classifier with the y_train as is.

Then you just scor what you obtain on the test using the classifier you have just fit with the measure that you prefer and you apply this thing to the transformation of X_test and y_test. I used the score function but i don't know what particular measure is the best, we have to check for it

```
print(tempclass.score(transformation.transform(X_test),y_test))
```

I also added a script to use the gridsearch if you want to use it. 
As you can see the things work in a same way, you set a key-value for the attributes, assign to a variable, fit the grid to the model with the transformation of x etc. I also added a function that print the best parameters found but i dont know how it is estimated that a partcular set is the best.
"""

start = time()
from sklearn.ensemble import RandomForestClassifier
tempclass = RandomForestClassifier(n_estimators= 100, max_depth= 10, max_features='auto', criterion='gini', class_weight={0:15, 1:85})
tempclass.fit(transformation.transform(X_train),y_train)
end = time()
print('\n' + str(end - start))

predicted = tempclass.predict(transformation.transform(X_test))

from sklearn.model_selection import GridSearchCV
t2 = RandomForestClassifier()
param_grid = {"n_estimators": [20],
              "max_depth": range(10, 25, 5),
              "max_features": ['auto', 'log2', 'sqrt'],
              "criterion": ['gini', 'entropy']}

# Applico la selezione GridSearchCV con 12 diverse configurazioni e cross validation 5-fold 
grid_search = GridSearchCV(t2, param_grid=param_grid, cv=3, n_jobs = -1)
# Alleno il modello sulla rappresentazione ottimizzata delle features del train
grid_search.fit(transformation.transform(X_train),y_train)

grid_search.best_params_

t2 = RandomForestClassifier(criterion = 'gini', max_depth = 10, max_features='auto', n_estimators=10)
t2.fit(transformation.transform(X_train),y_train)
predicted = t2.predict(transformation.transform(X_test))

print(t2.score(transformation.transform(X_test),y_test))

from sklearn import metrics
from sklearn.metrics import confusion_matrix

accuracy = metrics.accuracy_score(y_test, predicted)  
precision = metrics.precision_score(y_test, predicted, pos_label=1) 
recall = metrics.recall_score(y_test, predicted, pos_label=1) 
fmeasure = metrics.f1_score(y_test, predicted, pos_label=1)

folds = 1
print("Precision = " + str(precision/folds))
print("Accuracy = " + str(accuracy/folds))
print("Recall = " + str(recall/folds))
print("F-Measure = " + str(fmeasure/folds))

confusion_matrix(y_test, predicted)

"""#TEST STUFF (DONT CONSIDER THESE CELLS)"""

# OOH for sure
#df['Urban_or_Rural_Area'].unique()
# it must be ooh probably
#df['Police_Force'].unique()
#df['Number_of_Vehicles'].unique()
#df['Number_of_Casualties'].unique()
#df['Local_Authority_(District)'].unique()
# df['1st_Road_Class'].unique()
# df['1st_Road_Number'].unique()
#df['2nd_Road_Class'].unique()
#df['2nd_Road_Number'].unique()
# string
#df['Local_Authority_(Highway)'].unique()
#df['Road_Type'].unique()
#df['Junction_Control'].unique() # NAN
#df['Pedestrian_Crossing-Human_Control'].unique()
#df['Pedestrian_Crossing-Physical_Facilities'].unique()
#df['Weather_Conditions'].unique()
#df['Road_Surface_Conditions'].unique() # nan
#df['Special_Conditions_at_Site'].unique() # nan
#df['Carriageway_Hazards'].unique() #nan
#df['Did_Police_Officer_Attend_Scene_of_Accident'].unique() # nan
#df['Year'].unique()
# holds a natural order
#df['Speed_limit'].unique()
#df['Light_Conditions'].unique()

# ATTR THAT ARE PRETTY WELL DISTRIBUTED
#df['Urban_or_Rural_Area'].value_counts(normalize = True) # categorical
#df['Police_Force'].value_counts(normalize = True,dropna = False) # how to handle this?
#df['Local_Authority_(District)'].value_counts(normalize = True,dropna = False)
#df['1st_Road_Class'].value_counts(normalize = True,dropna = False)
#df['1st_Road_Number'].value_counts(normalize = True,dropna = False)
#df['Year'].value_counts(normalize = True,dropna = False)
#df['2nd_Road_Class'].value_counts(normalize = True,dropna = False)
#df['2nd_Road_Number'].value_counts(normalize = True,dropna = False)
#df['Local_Authority_(Highway)'].value_counts(normalize = True,dropna = False)




# ATTR THAT HAVE PROBLEMS WITH DATA
#df['Number_of_Vehicles'].value_counts(normalize = True, dropna = False) # 4 values hold 98% of data 1,2,... DONE
#df['Number_of_Casualties'].value_counts(normalize = True,dropna = False) # 4 values hold 98% of data 1,2,3+ DONE
#df['Road_Type'].value_counts(normalize = True,dropna = False) # unknown has really few observtions 1678 1,2, other? DONE
#df['Junction_Control'].value_counts(normalize = True,dropna = False) # NAN # almost 40% is nan impute using the neighbors #knn # ignored for now DONE 
#df['Pedestrian_Crossing-Physical_Facilities'].value_counts(normalize = True,dropna = False) # a value has more than 80% data. Merge all the others? DONE 

#df['Weather_Conditions'].value_counts(normalize = True,dropna = False) # one value 80% results, merge the others? 1,2, other DONE 
#df['Road_Surface_Conditions'].value_counts(normalize = True,dropna = False) # nan #two values have 96% of data and nan to be imputed (i think to the majority) 1,2,extreme conditions DONE
#df['Did_Police_Officer_Attend_Scene_of_Accident'].value_counts(normalize = True,dropna = False) # nan with 2 value, MERGE WITH NO DONE
#df['Speed_limit'].value_counts(normalize = True, dropna = False) # remove velocity 10. just one observation merge 10 and 20 DONE
#df['Light_Conditions'].value_counts(normalize = True,dropna = False) # maybe merge things to proper lightning and dark DONE
#df['LSOA_of_Accident_Location'].value_counts(normalize = True, dropna = False) DONE

df3 = imported_df[imported_df['Accident_Severity'] == 2]
df4 = imported_df[imported_df['Accident_Severity'] == 3]

plt.figure(figsize=(10, 8))
plt.scatter(df4['Location_Easting_OSGR'], df4['Location_Northing_OSGR'], c = 'green')
plt.scatter(df3['Location_Easting_OSGR'], df3['Location_Northing_OSGR'], c = 'red')