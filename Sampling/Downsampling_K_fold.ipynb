{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Downsampling K-fold",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kmNSDM9wBZH"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FllH47XeSGP3",
        "outputId": "a4eb8710-5663-43ec-ba23-d350f0ac4eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# General purpose\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Libraries for plotting\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Libraries for importing datasets from Google Drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Scikit-Learn libraries\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import resample\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "\n",
        "# If Google Colab does not have pygeohash\n",
        "!pip install pygeohash\n",
        "import pygeohash as gh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygeohash in /usr/local/lib/python3.6/dist-packages (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt4r2VFZwL3G"
      },
      "source": [
        "# Authentication for using Google Drive files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axm_Un_0SzH6"
      },
      "source": [
        "# Configuring Google Drive file loading (run it only once)\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7BYu4SKS1NX",
        "outputId": "3a301a89-5940-4490-c382-9933d6c1645f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1MURORv9iCRNNZtZORoNlv3WtDAYcGYqJ'\n",
        "id = link.split('=')[1]\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile(id)  \n",
        "imported_df = pd.read_csv(id, sep=',')\n",
        "\n",
        "df = imported_df.copy(deep = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7uc1QSw50xi"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_P0tszpGnQf"
      },
      "source": [
        "# for every column if it contains just one value (without add Nan to the count) drop it\n",
        "for col in df:\n",
        "  if not df[col].value_counts().to_list():\n",
        "    df.drop(col, axis = 1,inplace = True)\n",
        "\n",
        "# removig the index\n",
        "df.drop('Accident_Index',axis = 1, inplace = True)\n",
        "# removing useless columns (more than 90% of the data have the same value)\n",
        "df.drop('Special_Conditions_at_Site', axis = 1,inplace = True)\n",
        "df.drop('Carriageway_Hazards',axis = 1,inplace = True)\n",
        "df.drop('Pedestrian_Crossing-Human_Control',axis = 1, inplace= True)\n",
        "# removing duplicate of coordinate system\n",
        "df.drop('Location_Northing_OSGR',axis = 1, inplace = True)\n",
        "df.drop('Location_Easting_OSGR',axis = 1,inplace = True)\n",
        "df.drop('LSOA_of_Accident_Location',axis = 1,inplace = True)\n",
        "df.drop('1st_Road_Class',axis = 1, inplace = True)\n",
        "df.drop('1st_Road_Number',axis = 1, inplace = True)\n",
        "df.drop('2nd_Road_Class',axis = 1, inplace = True)\n",
        "df.drop('2nd_Road_Number',axis = 1, inplace = True)\n",
        "df.drop('Police_Force',axis = 1, inplace = True)\n",
        "df.drop('Local_Authority_(District)',axis = 1,inplace = True)\n",
        "\n",
        "# removing junctioncontrol because it has 40% of nan values\n",
        "df.drop('Junction_Control',axis = 1,inplace = True)\n",
        "# REMOVE ALSO THE FIRST AND SECOND ORDER STUFF BECAUSE THEY ARE USEFUL ONLY FOR JUNCTIONS\n",
        "\n",
        "# removing the date because we just use the day of the week\n",
        "df.drop('Date',inplace = True, axis = 1)\n",
        "\n",
        "# removing 13 nan rows in time, no way to impute it\n",
        "df.dropna(subset = ['Time'],inplace = True)\n",
        "df.dropna(subset = ['Did_Police_Officer_Attend_Scene_of_Accident'],inplace = True)\n",
        "\n",
        "# merging attribute values\n",
        "df.loc[df['Number_of_Vehicles'] > 2,'Number_of_Vehicles'] = '3+'\n",
        "df.loc[df['Number_of_Vehicles'] == 2,'Number_of_Vehicles'] = 'two'\n",
        "df.loc[df['Number_of_Vehicles'] == 1,'Number_of_Vehicles'] = 'one'\n",
        "df.loc[(df['Road_Type'] != 'Single carriageway') & (df['Road_Type'] != 'Dual carriageway'),'Road_Type'] = 'Other'\n",
        "df.loc[df['Pedestrian_Crossing-Physical_Facilities'] != 'No physical crossing within 50 meters', 'Pedestrian_Crossing-Physical_Facilities'] = 'Physical Crossing present'\n",
        "df.loc[(df['Weather_Conditions'] != 'Fine without high winds') & (df['Weather_Conditions'] != 'Raining without high winds'), 'Weather_Conditions'] = 'Other'\n",
        "df.loc[(df['Road_Surface_Conditions'] != 'Dry') & (df['Road_Surface_Conditions'] != 'Wet/Damp'),'Road_Surface_Conditions'] = 'Extreme_condition' \n",
        "df.loc[(df['Number_of_Casualties'] != 1) & (df['Number_of_Casualties'] != 2),'Number_of_Casualties'] = '3+'\n",
        "df.loc[df['Number_of_Casualties'] == 2,'Number_of_Casualties'] = 'two'\n",
        "df.loc[df['Number_of_Casualties'] == 1,'Number_of_Casualties'] = 'one' \n",
        "df.loc[(df['Light_Conditions'] == 'Daylight: Street light present') | (df['Light_Conditions'] == 'Darkness: Street lights present and lit'),'Light_Conditions'] = 'Proper lightning'\n",
        "df.loc[(df['Light_Conditions'] == 'Darkness: Street lights present but unlit') | (df['Light_Conditions'] == 'Darkeness: No street lighting'),'Light_Conditions'] = 'Insufficient lightning'\n",
        "df.loc[df['Light_Conditions'] == 'Darkness: Street lighting unknown','Light_Conditions'] = 'Unknown'\n",
        "df.loc[df['Speed_limit'] == 10,'Speed_limit'] = 20\n",
        "hours = pd.to_datetime(df['Time'], format='%H:%M').dt.hour\n",
        "df['Time'] = pd.cut(hours, \n",
        "                    bins=[0,6,12,18,24], \n",
        "                    include_lowest=True, \n",
        "                    labels=['Midnight','Morning','Evening','Night'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqWTA0ypqS5V"
      },
      "source": [
        "# DISCRETIZE COLUMNS\n",
        "df['Day_of_Week'] = df['Day_of_Week'].apply(str) # categorical\n",
        "df['Time'] = df['Time'].apply(str) # categorical\n",
        "df['Road_Type'] = df['Road_Type'].apply(str) # categorical\n",
        "df['Speed_limit'] = df['Speed_limit'].apply(str) # categorical\n",
        "df['Urban_or_Rural_Area'] = df['Urban_or_Rural_Area'].apply(str) # categorical\n",
        "df['Year'] = df['Year'].apply(str) # categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMLQy9mVwob-"
      },
      "source": [
        "#DISCRETIZE Longitude and Latitude using Geohash\n",
        "df['geohash']=df.apply(lambda x: gh.encode(x.Latitude, x.Longitude, precision=3), axis=1)\n",
        "df.drop('Longitude',axis = 1, inplace = True)\n",
        "df.drop('Latitude',axis = 1, inplace = True)\n",
        "\n",
        "features = df.drop('Accident_Severity',axis = 1).copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV6QC-ohmLDv"
      },
      "source": [
        "Apply one hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW74a4YqeLiN"
      },
      "source": [
        "# Save categorical variables\n",
        "categorical_features = features.select_dtypes(include = ['object','category']).columns\n",
        "\n",
        "# Categorical variables One Hot Encoding\n",
        "categorical_transformer = Pipeline(steps = [\n",
        "    ('onehot', preprocessing.OneHotEncoder())\n",
        "])\n",
        "\n",
        "numerical_features = features.select_dtypes(exclude = ['object','category']).columns\n",
        "\n",
        "\n",
        "numerical_transformer = Pipeline(steps = [\n",
        "    ('scaler',preprocessing.StandardScaler())\n",
        "])\n",
        "\n",
        "# Aggiungo le pipeline in un unico processo preprocessor\n",
        "preprocessor = ColumnTransformer( transformers = [\n",
        "    ('num',numerical_transformer, numerical_features),                                 \n",
        "    ('cat',categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# Create a model that contains all the possible features --> max variance\n",
        "transformation = make_pipeline(\n",
        "    preprocessor\n",
        ")\n",
        "\n",
        "transformation.fit(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWe00zf7mPux"
      },
      "source": [
        "# Dataset downsampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGV41kDxHaxb"
      },
      "source": [
        "# Downsampling\n",
        "\n",
        "df_fatal = df[df['Accident_Severity'] == 1]\n",
        "df_serious = df[df['Accident_Severity'] == 2]\n",
        "df_slight = df[df['Accident_Severity'] == 3]\n",
        "\n",
        "df_serious_resampled = resample(df_serious, n_samples=len(df_fatal), random_state=0)\n",
        "df_slight_resamples = resample(df_slight, n_samples=len(df_fatal), random_state=0)\n",
        "\n",
        "df_downsampled = pd.concat([df_fatal, df_serious_resampled, df_slight_resamples])\n",
        "df_downsampled = df_downsampled.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04kLKzaPjPVM"
      },
      "source": [
        "# Downsampling by merging Serious and Fatal\n",
        "\n",
        "df_fatal = df[df['Accident_Severity'] == 1]\n",
        "df_serious = df[df['Accident_Severity'] == 2]\n",
        "df_slight = df[df['Accident_Severity'] == 3]\n",
        "\n",
        "df_slight_resamples = resample(df_slight, n_samples=len(df_fatal)+len(df_serious), random_state=0)\n",
        "\n",
        "# merge the fatal and the serious together\n",
        "df_merged_downsampled = pd.concat([df_fatal, df_serious, df_slight_resamples])\n",
        "tmp = df_merged_downsampled['Accident_Severity'].copy()\n",
        "tmp[tmp == 2] = 1\n",
        "df_merged_downsampled['Accident_Severity'] = tmp\n",
        "df_merged_downsampled = df_merged_downsampled.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8ZiSrWExe9Y"
      },
      "source": [
        "**ATTENTION!**\n",
        "> DEFINE WHICH DATASET WILL BE USED BY USING ONLY 1 LINE AT A TIME\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG2naXCcrX0C"
      },
      "source": [
        "\n",
        "X = df_downsampled\n",
        "#X = df_merged_downsampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ-MrZ3gobrF"
      },
      "source": [
        "# Main program\n",
        "## Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSLIhKZfofv5"
      },
      "source": [
        "#Use only one clf at a time by commenting and uncommenting the correct lines\n",
        "\n",
        "#################################### Stohastic SVM ####################################################\n",
        "#Best parameter for df_downsampled dataset\n",
        "clf = SGDClassifier(loss=\"hinge\", n_iter_no_change=100, alpha=0.001, max_iter=2000)\n",
        "\n",
        "#Best parameter for df_merged_downsampled dataset\n",
        "#clf = SGDClassifier(loss=\"hinge\", n_iter_no_change=100, alpha=0.000001, max_iter = 10000)\n",
        "########################################################################################################\n",
        "\n",
        "##################################### RandomForest #####################################################\n",
        "#Best parameter for df_downsampled dataset\n",
        "#clf = RandomForestClassifier(n_estimators= 200, max_depth= 30, max_features='auto', criterion='entropy')\n",
        "\n",
        "#Best parameter for df_merged_downsampled dataset\n",
        "#clf = RandomForestClassifier(n_estimators= 200, max_depth= 30, max_features='sqrt', criterion='gini')\n",
        "########################################################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHZAR0T6qLDO"
      },
      "source": [
        "Run the model with K-fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku_DHhKXqe5D"
      },
      "source": [
        "accuracy = precision = recall = fmeasure = count = 0\n",
        "\n",
        "#number of folds\n",
        "K = 10\n",
        "\n",
        "kf = KFold(n_splits=K)\n",
        "kf.get_n_splits(X)\n",
        "for train_index, test_index in kf.split(X):\n",
        "  count += 1\n",
        "\n",
        "  X_train = X.iloc[train_index].drop('Accident_Severity',axis = 1).copy()\n",
        "  X_test = X.iloc[test_index].drop('Accident_Severity',axis = 1).copy()\n",
        "  Y_train = X.iloc[train_index]['Accident_Severity']\n",
        "  Y_test = X.iloc[test_index]['Accident_Severity']\n",
        "  clf.fit(transformation.transform(X_train), Y_train)\n",
        "\n",
        "  predicted = clf.predict(transformation.transform(X_test))\n",
        "\n",
        "  accuracy += metrics.accuracy_score(Y_test, predicted)  \n",
        "  precision += metrics.precision_score(Y_test, predicted, pos_label=1, average = None) \n",
        "  recall += metrics.recall_score(Y_test, predicted, pos_label=1, average = None) \n",
        "  fmeasure += metrics.f1_score(Y_test, predicted, pos_label=1, average = None)\n",
        "  print(\"Fold \"+str(count)+\" finished\")\n",
        "\n",
        "print(\"Precision = \" + str(precision/K))\n",
        "print(\"Accuracy = \" + str(accuracy/K))\n",
        "print(\"Recall = \" + str(recall/K))\n",
        "print(\"F-Measure = \" + str(fmeasure/K))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0J70koHyaB9"
      },
      "source": [
        "## Grid search\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTouIzajzXwc"
      },
      "source": [
        "Defining train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECF9ahlszXCF"
      },
      "source": [
        "train_df, test_df = train_test_split(new_df,test_size = 0.2)\n",
        "X_train = train_df.drop('Accident_Severity',axis = 1).copy()\n",
        "Y_train = train_df['Accident_Severity'].copy()\n",
        "X_test = test_df.drop('Accident_Severity',axis = 1).copy()\n",
        "Y_test = test_df['Accident_Severity'].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLzxNZntz1c3"
      },
      "source": [
        "Grid search for SGDC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxJeSSEyyZID"
      },
      "source": [
        "param_grid = {\n",
        "    'penalty': [\"l2\", \"l1\", \"elasticnet\"],\n",
        "    'alpha': [10 ** x for x in range(-7, 1)],\n",
        "    'l1_ratio': [0, 0.05, 0.1, 0.2, 0.5, 0.8, 0.9, 0.95, 1],\n",
        "    'n_iter_no_change': [100, 500, 1000]\n",
        "}\n",
        "clf = SGDClassifier(loss=\"hinge\",  max_iter=4000)\n",
        "\n",
        "clf_grid = GridSearchCV(estimator=clf, param_grid=param_grid,\n",
        "                                    n_jobs=-1, cv=3)\n",
        "\n",
        "clf_grid.fit(transformation.transform(X_train), Y_train)\n",
        "\n",
        "clf_grid.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi7OlVskyjCq"
      },
      "source": [
        "Grid search for RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbthdrIjyin1"
      },
      "source": [
        "param_grid = {\"n_estimators\":[100, 200],\n",
        "              \"max_depth\": [30],\n",
        "              \"max_features\": ['auto', 'log2', 'sqrt'],\n",
        "              \"criterion\": ['gini','entropy']}\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "clf_grid = GridSearchCV(t2, param_grid=param_grid, cv=3, n_jobs = -1)\n",
        "\n",
        "clf_grid.fit(transformation.transform(X_train),y_train)\n",
        "\n",
        "clf_grid.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}